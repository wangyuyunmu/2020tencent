{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: keras==2.2.4 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (2.2.4)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.0.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.1.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (1.16.4)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras==2.2.4) (5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==2.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already up-to-date: keras_applications in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras_applications) (2.10.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras_applications) (1.16.4)\n",
      "Requirement already satisfied, skipping upgrade: six in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from h5py->keras_applications) (1.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade keras_applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,Adam,RMSprop\n",
    "from keras.layers import *\n",
    "import os\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, **kwargs):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._model_dim = model_dim\n",
    "        super(Embedding, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self._vocab_size, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            name=\"embeddings\")\n",
    "        super(Embedding, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "        embeddings = K.gather(self.embeddings, inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        return embeddings\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return input_shape + (self._model_dim,)\n",
    "    \n",
    "class Add(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(Add, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_a, input_b = inputs\n",
    "        return input_a + input_b\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0]\n",
    "    \n",
    "class ScaledDotProductAttention(Layer):\n",
    "\n",
    "    def __init__(self, masking=True, future=False, dropout_rate=0., **kwargs):\n",
    "        self._masking = masking\n",
    "        self._future = future\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._masking_num = -2**32+1\n",
    "        super(ScaledDotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def mask(self, inputs, masks):\n",
    "        masks = K.cast(masks, 'float32')\n",
    "        masks = K.tile(masks, [K.shape(inputs)[0] // K.shape(masks)[0], 1])\n",
    "        masks = K.expand_dims(masks, 1)\n",
    "        outputs = inputs + masks * self._masking_num\n",
    "        return outputs\n",
    "    \n",
    "    def future_mask(self, inputs):\n",
    "        diag_vals = tf.ones_like(inputs[0, :, :])\n",
    "        tril = tf.linalg.LinearOperatorLowerTriangular(diag_vals).to_dense()  \n",
    "        future_masks = tf.tile(tf.expand_dims(tril, 0), [tf.shape(inputs)[0], 1, 1])\n",
    "        paddings = tf.ones_like(future_masks) * self._masking_num\n",
    "        outputs = tf.where(tf.equal(future_masks, 0), paddings, inputs)\n",
    "        return outputs\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._masking:\n",
    "            assert len(inputs) == 4, \"inputs should be set [queries, keys, values, masks].\"\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            assert len(inputs) == 3, \"inputs should be set [queries, keys, values].\"\n",
    "            queries, keys, values = inputs\n",
    "\n",
    "        if K.dtype(queries) != 'float32':  queries = K.cast(queries, 'float32')\n",
    "        if K.dtype(keys) != 'float32':  keys = K.cast(keys, 'float32')\n",
    "        if K.dtype(values) != 'float32':  values = K.cast(values, 'float32')\n",
    "\n",
    "        matmul = K.batch_dot(queries, tf.transpose(keys, [0, 2, 1])) # MatMul\n",
    "        scaled_matmul = matmul / int(queries.shape[-1]) ** 0.5  # Scale\n",
    "        if self._masking:\n",
    "            scaled_matmul = self.mask(scaled_matmul, masks) # Mask(opt.)\n",
    "\n",
    "        if self._future:\n",
    "            scaled_matmul = self.future_mask(scaled_matmul)\n",
    "\n",
    "        softmax_out = K.softmax(scaled_matmul) # SoftMax\n",
    "        # Dropout\n",
    "        out = K.dropout(softmax_out, self._dropout_rate)\n",
    "        \n",
    "        outputs = K.batch_dot(out, values)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "    \n",
    "class PositionEncoding(Layer):\n",
    "\n",
    "    def __init__(self, model_dim, **kwargs):\n",
    "        self._model_dim = model_dim\n",
    "        super(PositionEncoding, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape[1]\n",
    "        position_encodings = np.zeros((seq_length, self._model_dim))\n",
    "        for pos in range(seq_length):\n",
    "            for i in range(self._model_dim):\n",
    "                position_encodings[pos, i] = pos / np.power(10000, (i-i%2) / self._model_dim)\n",
    "        position_encodings[:, 0::2] = np.sin(position_encodings[:, 0::2]) # 2i\n",
    "        position_encodings[:, 1::2] = np.cos(position_encodings[:, 1::2]) # 2i+1\n",
    "        position_encodings = K.cast(position_encodings, 'float32')\n",
    "        return position_encodings\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    def __init__(self, n_heads, head_dim, dropout_rate=.1, masking=True, future=False, trainable=True, **kwargs):\n",
    "        self._n_heads = n_heads\n",
    "        self._head_dim = head_dim\n",
    "        self._dropout_rate = dropout_rate\n",
    "        self._masking = masking\n",
    "        self._future = future\n",
    "        self._trainable = trainable\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self._weights_queries = self.add_weight(\n",
    "            shape=(input_shape[0][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_queries')\n",
    "        self._weights_keys = self.add_weight(\n",
    "            shape=(input_shape[1][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_keys')\n",
    "        self._weights_values = self.add_weight(\n",
    "            shape=(input_shape[2][-1], self._n_heads * self._head_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name='weights_values')\n",
    "        super(MultiHeadAttention, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._masking:\n",
    "            assert len(inputs) == 4, \"inputs should be set [queries, keys, values, masks].\"\n",
    "            queries, keys, values, masks = inputs\n",
    "        else:\n",
    "            assert len(inputs) == 3, \"inputs should be set [queries, keys, values].\"\n",
    "            queries, keys, values = inputs\n",
    "        \n",
    "        queries_linear = K.dot(queries, self._weights_queries) \n",
    "        keys_linear = K.dot(keys, self._weights_keys)\n",
    "        values_linear = K.dot(values, self._weights_values)\n",
    "\n",
    "        queries_multi_heads = tf.concat(tf.split(queries_linear, self._n_heads, axis=2), axis=0)\n",
    "        keys_multi_heads = tf.concat(tf.split(keys_linear, self._n_heads, axis=2), axis=0)\n",
    "        values_multi_heads = tf.concat(tf.split(values_linear, self._n_heads, axis=2), axis=0)\n",
    "        \n",
    "        if self._masking:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads, masks]\n",
    "        else:\n",
    "            att_inputs = [queries_multi_heads, keys_multi_heads, values_multi_heads]\n",
    "            \n",
    "        attention = ScaledDotProductAttention(\n",
    "            masking=self._masking, future=self._future, dropout_rate=self._dropout_rate)\n",
    "        att_out = attention(att_inputs)\n",
    "\n",
    "        outputs = tf.concat(tf.split(att_out, self._n_heads, axis=0), axis=2)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(Layer):\n",
    "    \n",
    "    def __init__(self, model_dim, inner_dim, trainable=True, **kwargs):\n",
    "        self._model_dim = model_dim\n",
    "        self._inner_dim = inner_dim\n",
    "        self._trainable = trainable\n",
    "        super(PositionWiseFeedForward, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_inner = self.add_weight(\n",
    "            shape=(input_shape[-1], self._inner_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"weights_inner\")\n",
    "        self.weights_out = self.add_weight(\n",
    "            shape=(self._inner_dim, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"weights_out\")\n",
    "        self.bais_inner = self.add_weight(\n",
    "            shape=(self._inner_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"bais_inner\")\n",
    "        self.bais_out = self.add_weight(\n",
    "            shape=(self._model_dim,),\n",
    "            initializer='uniform',\n",
    "            trainable=self._trainable,\n",
    "            name=\"bais_out\")\n",
    "        super(PositionWiseFeedForward, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'float32':\n",
    "            inputs = K.cast(inputs, 'float32')\n",
    "        inner_out = K.relu(K.dot(inputs, self.weights_inner) + self.bais_inner)\n",
    "        outputs = K.dot(inner_out, self.weights_out) + self.bais_out\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return self._model_dim\n",
    "\n",
    "    \n",
    "class LayerNormalization(Layer):\n",
    "\n",
    "    def __init__(self, epsilon=1e-8, **kwargs):\n",
    "        self._epsilon = epsilon\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.beta = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='zero',\n",
    "            name='beta')\n",
    "        self.gamma = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer='one',\n",
    "            name='gamma')\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        mean, variance = tf.nn.moments(inputs, [-1], keepdims=True)\n",
    "        normalized = (inputs - mean) / ((variance + self._epsilon) ** 0.5)\n",
    "        outputs = self.gamma * normalized + self.beta\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "class Transformer(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, model_dim, \n",
    "            n_heads=8, encoder_stack=6, decoder_stack=6, feed_forward_size=2048, dropout_rate=0.1, **kwargs):\n",
    "        self._vocab_size = vocab_size\n",
    "        self._model_dim = model_dim\n",
    "        self._n_heads = n_heads\n",
    "        self._encoder_stack = encoder_stack\n",
    "        self._decoder_stack = decoder_stack\n",
    "        self._feed_forward_size = feed_forward_size\n",
    "        self._dropout_rate = dropout_rate\n",
    "        super(Transformer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.embeddings = self.add_weight(\n",
    "            shape=(self._vocab_size, self._model_dim),\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name=\"embeddings\")\n",
    "        super(Transformer, self).build(input_shape)\n",
    "\n",
    "\n",
    "    def encoder(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "\n",
    "        masks = K.equal(inputs, 0)\n",
    "        # Embeddings\n",
    "        embeddings = K.gather(self.embeddings, inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        # Position Encodings\n",
    "        position_encodings = PositionEncoding(self._model_dim)(embeddings)\n",
    "        # Embedings + Postion-encodings\n",
    "        encodings = embeddings + position_encodings\n",
    "        # Dropout\n",
    "        encodings = K.dropout(encodings, self._dropout_rate)\n",
    "\n",
    "        for i in range(self._encoder_stack):\n",
    "            # Multi-head-Attention\n",
    "            attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads)\n",
    "            attention_input = [encodings, encodings, encodings, masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            # Add & Norm\n",
    "            attention_out += encodings\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "            # Feed-Forward\n",
    "            ff = PositionWiseFeedForward(self._model_dim, self._feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "            # Add & Norm\n",
    "            ff_out += attention_out\n",
    "            encodings = LayerNormalization()(ff_out)\n",
    "\n",
    "        return encodings, masks\n",
    "\n",
    "\n",
    "    def decoder(self, inputs):\n",
    "        decoder_inputs, encoder_encodings, encoder_masks = inputs\n",
    "        if K.dtype(decoder_inputs) != 'int32':\n",
    "            decoder_inputs = K.cast(decoder_inputs, 'int32')\n",
    "\n",
    "        decoder_masks = K.equal(decoder_inputs, 0)\n",
    "        # Embeddings\n",
    "        embeddings = K.gather(self.embeddings, decoder_inputs)\n",
    "        embeddings *= self._model_dim ** 0.5 # Scale\n",
    "        # Position Encodings\n",
    "        position_encodings = PositionEncoding(self._model_dim)(embeddings)\n",
    "        # Embedings + Postion-encodings\n",
    "        encodings = embeddings + position_encodings\n",
    "        # Dropout\n",
    "        encodings = K.dropout(encodings, self._dropout_rate)\n",
    "        \n",
    "        for i in range(self._decoder_stack):\n",
    "            # Masked-Multi-head-Attention\n",
    "            masked_attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads, future=True)\n",
    "            masked_attention_input = [encodings, encodings, encodings, decoder_masks]\n",
    "            masked_attention_out = masked_attention(masked_attention_input)\n",
    "            # Add & Norm\n",
    "            masked_attention_out += encodings\n",
    "            masked_attention_out = LayerNormalization()(masked_attention_out)\n",
    "\n",
    "            # Multi-head-Attention\n",
    "            attention = MultiHeadAttention(self._n_heads, self._model_dim // self._n_heads)\n",
    "            attention_input = [masked_attention_out, encoder_encodings, encoder_encodings, encoder_masks]\n",
    "            attention_out = attention(attention_input)\n",
    "            # Add & Norm\n",
    "            attention_out += masked_attention_out\n",
    "            attention_out = LayerNormalization()(attention_out)\n",
    "\n",
    "            # Feed-Forward\n",
    "            ff = PositionWiseFeedForward(self._model_dim, self._feed_forward_size)\n",
    "            ff_out = ff(attention_out)\n",
    "            # Add & Norm\n",
    "            ff_out += attention_out\n",
    "            encodings = LayerNormalization()(ff_out)\n",
    "\n",
    "        # Pre-Softmax 与 Embeddings 共享参数\n",
    "        linear_projection = K.dot(encodings, K.transpose(self.embeddings))\n",
    "        outputs = K.softmax(linear_projection)\n",
    "        return outputs\n",
    "\n",
    "    def call(self, inputs):\n",
    "        encoder_inputs, decoder_inputs = inputs\n",
    "        encoder_encodings, encoder_masks = self.encoder(encoder_inputs)\n",
    "        encoder_outputs = self.decoder([decoder_inputs, encoder_encodings, encoder_masks])\n",
    "        return encoder_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return  (input_shape[0][0], input_shape[0][1], self._vocab_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-2-78e7be21646f>:59: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_1 (Transformer)     (None, 256, 5000)    2560000     encoder_inputs[0][0]             \n",
      "                                                                 decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 32)           642176      transformer_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,202,176\n",
      "Trainable params: 3,202,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Input\n",
    "# from tensorflow.keras.utils import plot_model\n",
    "\n",
    "vocab_size = 5000\n",
    "max_seq_len = 256 \n",
    "model_dim = 512\n",
    "\n",
    "vocab_size = 32\n",
    "max_seq_len = 256 \n",
    "model_dim = 5000\n",
    "\n",
    "encoder_inputs = Input(shape=(max_seq_len,), name='encoder_inputs')\n",
    "decoder_inputs = Input(shape=(max_seq_len,), name='decoder_inputs')\n",
    "tran_outputs = Transformer(vocab_size, model_dim)([encoder_inputs, decoder_inputs])\n",
    "\n",
    "outputs = Bidirectional(LSTM(16,activation='softsign',return_sequences=False))(tran_outputs)\n",
    "\n",
    "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buid_model():\n",
    "    \n",
    "    vocab_size = 5000\n",
    "    max_seq_len = 200 \n",
    "    model_dim = 512\n",
    "    \n",
    "    S_inputs = Input(shape=(200,),name='main_input',dtype='int32')\n",
    "#     with h5py.File('../../get_w2v_feat/w2v1_pre_ad/embeddings_matrix_ad_size_32_w100_count_0.h5','r') as f:\n",
    "#         embeddings_matrix = np.array(f.get('embeddings_matrix'))\n",
    "    \n",
    "\n",
    "#     EMBEDDING_DIM = 32 #词向量维度\n",
    "#     MAX_SEQUENCE_LENGTH = 200\n",
    "#     embeddings = Embedding(input_dim = len(embeddings_matrix), # 字典长度\n",
    "#                                 output_dim = EMBEDDING_DIM, # 词向量 长度（100）\n",
    "#                                 weights=[embeddings_matrix], # 重点：预训练的词向量系数\n",
    "#                                 input_length=MAX_SEQUENCE_LENGTH, # 每句话的 最大长度（必须padding） \n",
    "#                                 trainable=False, # 是否在 训练的过程中 更新词向量\n",
    "#                                 mask_zero = True)(S_inputs)\n",
    "    \n",
    "#     embeddings = np.random.randint(0,model_dim,(2000,200))\n",
    "    outputs = Transformer(vocab_size, model_dim)([S_inputs, S_inputs])\n",
    "#     outputs = Bidirectional(LSTM(32,activation='softsign',return_sequences=False))(outputs)\n",
    "    outputs = GlobalMaxPool1D()(outputs)\n",
    "    outputs = Dense(10, activation='softmax', name='main_output')(outputs)\n",
    "    model = keras.models.Model(inputs=[S_inputs], outputs=outputs)\n",
    "    \n",
    "#     O_seq = Bidirectional(LSTM(32,activation='softsign',return_sequences=False))(embeddings)\n",
    "#     O_seq = Attention()(O_seq)\n",
    "#     O_seq = Bidirectional(LSTM(16,activation='softsign',return_sequences=False))(O_seq)\n",
    "#     O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "#     O_seq = GlobalMaxPool1D()(O_seq)\n",
    "#     outputs = Dense(10, activation='softmax', name='main_output')(O_seq)\n",
    "\n",
    "    # 定义一个具有两个输入输出的模型\n",
    "#     model = keras.models.Model(inputs=[S_inputs],#,auxiliary_input],\n",
    "#                                outputs=[outputs])  # 这里的输入输出顺序与fit时一致就好\n",
    "    \n",
    "#     model.layers[1].trainable = False\n",
    "    \n",
    "#     opt = RMSprop(lr=0.01,  clipnorm=1.0)\n",
    "    opt = Adam(lr=0.01)\n",
    "    model.compile(optimizer=opt,\n",
    "                  sample_weight_mode='None',#\"temporal\",\n",
    "                  loss={'main_output': 'categorical_crossentropy'},\n",
    "                 metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "#     print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_load():\n",
    "    print('loading data ... \\n')\n",
    "\n",
    "    with h5py.File('../../get_w2v_feat/w2v1_pre_ad/word_train_ad_w2v.h5', 'r') as f:\n",
    "        data = np.array(f.get('word_data'))\n",
    "\n",
    "    label = pd.read_csv('../../train_preliminary/user.csv').sort_values(by=['user_id'])\n",
    "\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, label, test_size=0.2, random_state=2020)\n",
    "\n",
    "    train_y_age = train_y['age'].values - 1\n",
    "    train_y_age = keras.utils.np_utils.to_categorical(train_y_age, num_classes=10)\n",
    "    train_y_gender = train_y['gender'].values - 1\n",
    "\n",
    "    test_y_age = test_y['age'].values - 1\n",
    "    test_y_age = keras.utils.np_utils.to_categorical(test_y_age, num_classes=10)\n",
    "    test_y_gender = test_y['gender'].values - 1\n",
    "\n",
    "    print('get data ... \\n')\n",
    "\n",
    "    return train_x, test_x, train_y_age, train_y_gender,test_y_age,test_y_gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "main_input (InputLayer)         (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "transformer_3 (Transformer)     (None, 200, 5000)    2560000     main_input[0][0]                 \n",
      "                                                                 main_input[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 5000)         0           transformer_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 10)           50010       global_max_pooling1d_1[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,610,010\n",
      "Trainable params: 2,610,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "lstm model geted...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = buid_model()\n",
    "print('lstm model geted...\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ... \n",
      "\n",
      "get data ... \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x, test_x, train_y_age, train_y_gender,test_y_age,test_y_gender = data_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm model fit...\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 2000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n"
     ]
    }
   ],
   "source": [
    "def get_filename_for_saving(save_dir):\n",
    "    return os.path.join(save_dir,\n",
    "                        \"self_lstm_attention_max_dense_ad_age_adm_{val_loss:.3f}-{val_acc:.3f}-{epoch:03d}-{loss:.3f}-{acc:.3f}.hdf5\")\n",
    "\n",
    "print('lstm model fit...\\n')\n",
    "checkpointer = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=get_filename_for_saving(''),\n",
    "    save_best_only=False)\n",
    "stopping = keras.callbacks.EarlyStopping(patience=8)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(factor=0.1, patience=2, min_lr=0.0001)\n",
    "\n",
    "# with h5py.File('../train_data_weight.h5', 'r') as f:\n",
    "#         weight = np.array(f.get('weight'))\n",
    "# train_w, test_w= train_test_split(weight, test_size=0.2, random_state=2020)\n",
    "\n",
    "# train_w = np.squeeze(train_w)\n",
    "# t_w = 10/np.log(train_w)\n",
    "\n",
    "test_x = train_x = np.random.randint(0,16,(2000,200))\n",
    "train_y_age = np.random.randint(0,10,(2000,1))\n",
    "train_y_age = test_y_age = keras.utils.np_utils.to_categorical(train_y_age, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit({'main_input': train_x },#,'aux_input': train_x_sta},\n",
    "          {'main_output': train_y_age},\n",
    "          epochs=100,\n",
    "          batch_size=256,\n",
    "          validation_data=({'main_input': test_x},#,'aux_input': test_x_sta},\n",
    "                           {'main_output': test_y_age}),\n",
    "          callbacks=[checkpointer, reduce_lr, stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting keras-transformer\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/8a/2b/c465241bd3f37a3699246827ff4ad7974c6edeaa69cf9cdcff2fd1d3ba46/keras-transformer-0.37.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras-transformer) (1.16.4)\n",
      "Requirement already satisfied: Keras in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from keras-transformer) (2.2.4)\n",
      "Collecting keras-pos-embd>=0.11.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz (5.9 kB)\n",
      "Collecting keras-multi-head>=0.27.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz (14 kB)\n",
      "Collecting keras-layer-normalization>=0.14.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\n",
      "Collecting keras-position-wise-feed-forward>=0.6.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\n",
      "Collecting keras-embed-sim>=0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz (4.1 kB)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (5.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (1.4.1)\n",
      "Requirement already satisfied: h5py in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (2.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from Keras->keras-transformer) (1.0.8)\n",
      "Collecting keras-self-attention==0.46.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz (10 kB)\n",
      "Building wheels for collected packages: keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
      "  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.37.0-py3-none-any.whl size=12941 sha256=3d8488fa9cd4b78419cc5e7ed2d615c8d732db7c87aefd09564562cef86f5168\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/84/58/b8/d3d3955c4d41daee7d2d793b0de37973a059db7bac53746c07\n",
      "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7553 sha256=5291c2742845d3b2c71ca1d8d41fec915255a91bffee56bbb8c5f914262c6709\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/24/3d/17/e81ba1a5486ee02757711dd9f84b62cf6396b6f30a8ae51a19\n",
      "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-py3-none-any.whl size=15613 sha256=082e02a933ac1fb487c9329fab95cd0eab43533cf8a834ad4543f5031102959a\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/82/a3/cb/cc136aa60959275918dd9059a2cb42e1cd29c96552212ee9db\n",
      "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=e40a81510a5fc871c7a841749f1373482b5a5693c64cc8a69bc042f639525fb0\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/d1/f7/08/a499940b0159c4694065705422356b7b0dbc7ac2eb80de4a68\n",
      "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=62da1bf7261dfc8e23a83e62d879bf9c3130a02e02c0ab84d947d37f62a2ab03\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/a9/54/e5/3c88bc4135aef9935fd1a02f2990f37ed477f4056f45ea2bbf\n",
      "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-py3-none-any.whl size=4674 sha256=284b6ec9fa0bc6ddf9349873789b0d8d738c4b0af142e4be250c5786e165fdd0\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/e6/9d/3d/d28360b51b00d663ed85f927b4000028a16cf27550e32edf98\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-py3-none-any.whl size=17278 sha256=e80d2f7b32d1013807dcab1ca0203816b54efea6cc0aba2061da99ac70694ff3\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/e7/a6/9f/8c92b96b867dbaabe73279acb41670c824ee78fe43cea743de\n",
      "Successfully built keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
      "Installing collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer\n",
      "Successfully installed keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.37.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras_transformer import get_model\n",
    "\n",
    "# Build a small toy token dictionary\n",
    "tokens = 'all work and no play makes jack a dull boy'.split(' ')\n",
    "token_dict = {\n",
    "    '<PAD>': 0,\n",
    "    '<START>': 1,\n",
    "    '<END>': 2,\n",
    "}\n",
    "for token in tokens:\n",
    "    if token not in token_dict:\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<START>': 1,\n",
       " '<END>': 2,\n",
       " 'all': 3,\n",
       " 'work': 4,\n",
       " 'and': 5,\n",
       " 'no': 6,\n",
       " 'play': 7,\n",
       " 'makes': 8,\n",
       " 'jack': 9,\n",
       " 'a': 10,\n",
       " 'dull': 11,\n",
       " 'boy': 12}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy data\n",
    "encoder_inputs_no_padding = []\n",
    "encoder_inputs, decoder_inputs, decoder_outputs = [], [], []\n",
    "for i in range(1, len(tokens) - 1):\n",
    "    encode_tokens, decode_tokens = tokens[:i], tokens[i:]\n",
    "    encode_tokens = ['<START>'] + encode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(encode_tokens))\n",
    "    output_tokens = decode_tokens + ['<END>', '<PAD>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))\n",
    "    decode_tokens = ['<START>'] + decode_tokens + ['<END>'] + ['<PAD>'] * (len(tokens) - len(decode_tokens))\n",
    "    encode_tokens = list(map(lambda x: token_dict[x], encode_tokens))\n",
    "    decode_tokens = list(map(lambda x: token_dict[x], decode_tokens))\n",
    "    output_tokens = list(map(lambda x: [token_dict[x]], output_tokens))\n",
    "    encoder_inputs_no_padding.append(encode_tokens[:i + 2])\n",
    "    encoder_inputs.append(encode_tokens)\n",
    "    decoder_inputs.append(decode_tokens)\n",
    "    decoder_outputs.append(output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 3, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 3, 4, 5, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 3, 4, 5, 6, 2, 0, 0, 0, 0, 0, 0],\n",
       " [1, 3, 4, 5, 6, 7, 2, 0, 0, 0, 0, 0],\n",
       " [1, 3, 4, 5, 6, 7, 8, 2, 0, 0, 0, 0],\n",
       " [1, 3, 4, 5, 6, 7, 8, 9, 2, 0, 0, 0],\n",
       " [1, 3, 4, 5, 6, 7, 8, 9, 10, 2, 0, 0]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 2, 0],\n",
       " [1, 5, 6, 7, 8, 9, 10, 11, 12, 2, 0, 0],\n",
       " [1, 6, 7, 8, 9, 10, 11, 12, 2, 0, 0, 0],\n",
       " [1, 7, 8, 9, 10, 11, 12, 2, 0, 0, 0, 0],\n",
       " [1, 8, 9, 10, 11, 12, 2, 0, 0, 0, 0, 0],\n",
       " [1, 9, 10, 11, 12, 2, 0, 0, 0, 0, 0, 0],\n",
       " [1, 10, 11, 12, 2, 0, 0, 0, 0, 0, 0, 0],\n",
       " [1, 11, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Token-Embedding (EmbeddingRet)  [(None, None, 30), ( 390         Encoder-Input[0][0]              \n",
      "                                                                 Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Embedding (TrigPosEmbed (None, None, 30)     0           Token-Embedding[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 30)     3720        Encoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-Embedding[0][0]          \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, None, 30)     60          Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, None, 30)     7350        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, None, 30)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, None, 30)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, None, 30)     60          Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 30)     3720        Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, None, 30)     60          Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, None, 30)     7350        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, None, 30)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, None, 30)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, None, 30)     60          Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 30)     3720        Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 30)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Embedding (TrigPosEmbed (None, None, 30)     0           Token-Embedding[1][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, None, 30)     60          Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 30)     3720        Decoder-Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, None, 30)     7350        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 30)     0           Decoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, None, 30)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 30)     0           Decoder-Embedding[0][0]          \n",
      "                                                                 Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, None, 30)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadSelfAttentio (None, None, 30)     60          Decoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, None, 30)     60          Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 30)     3720        Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 30)     0           Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 30)     0           Decoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-MultiHeadQueryAttenti (None, None, 30)     60          Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward (FeedForw (None, None, 30)     7350        Decoder-1-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Dropout ( (None, None, 30)     0           Decoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Add (Add) (None, None, 30)     0           Decoder-1-MultiHeadQueryAttention\n",
      "                                                                 Decoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-1-FeedForward-Norm (Lay (None, None, 30)     60          Decoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 30)     3720        Decoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 30)     0           Decoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 30)     0           Decoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadSelfAttentio (None, None, 30)     60          Decoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 30)     3720        Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 30)     0           Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 30)     0           Decoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-MultiHeadQueryAttenti (None, None, 30)     60          Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward (FeedForw (None, None, 30)     7350        Decoder-2-MultiHeadQueryAttention\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Dropout ( (None, None, 30)     0           Decoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Add (Add) (None, None, 30)     0           Decoder-2-MultiHeadQueryAttention\n",
      "                                                                 Decoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Decoder-2-FeedForward-Norm (Lay (None, None, 30)     60          Decoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Output (EmbeddingSim)   (None, None, 13)     13          Decoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Token-Embedding[1][1]            \n",
      "==================================================================================================\n",
      "Total params: 63,913\n",
      "Trainable params: 63,523\n",
      "Non-trainable params: 390\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected Decoder-Output to have 3 dimensions, but got array with shape (8000, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-7e56b9e58e31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     y=np.asarray(decoder_outputs * 1000),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected Decoder-Output to have 3 dimensions, but got array with shape (8000, 1)"
     ]
    }
   ],
   "source": [
    "S_inputs = Input(shape=(200,),name='main_input',dtype='int32')\n",
    "\n",
    "# Build the model\n",
    "model = get_model(\n",
    "    token_num=len(token_dict),\n",
    "    embed_dim=30,\n",
    "    encoder_num=3,\n",
    "    decoder_num=2,\n",
    "    head_num=3,\n",
    "    hidden_dim=120,\n",
    "    attention_activation='relu',\n",
    "    feed_forward_activation='relu',\n",
    "    dropout_rate=0.05,\n",
    "    embed_weights=np.random.random((13, 30)),\n",
    ")\n",
    "tran_out = model(x=[np.asarray(encoder_inputs * 1000), np.asarray(decoder_inputs * 1000)],\n",
    "    y=np.asarray(decoder_outputs * 1000))\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    ")\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    x=[np.asarray(encoder_inputs * 1000), np.asarray(decoder_inputs * 1000)],\n",
    "    y=np.asarray(decoder_outputs * 1000),\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Collecting transformers\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/48/35/ad2c5b1b8f99feaaf9d7cdadaeef261f098c6e1a6a2935d4d07662a6b780/transformers-2.11.0-py3-none-any.whl (674 kB)\n",
      "\u001b[K     |████████████████████████████████| 674 kB 799 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tokenizers==0.7.0\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 1.6 MB/s eta 0:00:01     |██████████████▉                 | 1.7 MB 1.6 MB/s eta 0:00:02     |█████████████████████████▉      | 3.0 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from transformers) (1.16.4)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from transformers) (2.20.1)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/1a/a1/6d8fdf4a20ffbbf2bd6003dff47a0628b9e6a4b840c421b0dec27da9376e/regex-2020.6.8-cp36-cp36m-manylinux2010_x86_64.whl (660 kB)\n",
      "\u001b[K     |███████████████████████████████▊| 655 kB 1.1 MB/s eta 0:00:01     |████████████████████████████████| 660 kB 1.1 MB/s \n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/93/83/71a2ee6158bb9f39a90c0dea1637f81d5eef866e188e1971a1b1ab01a35a/filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883 kB)\n",
      "\u001b[K     |████████████████████████████████| 883 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl (18 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/68/e5/0366f50a00db181f4b7f3bdc408fc7c4177657f5bf45cb799b79fb4ce15c/sentencepiece-0.1.92-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from transformers) (4.42.1)\n",
      "Collecting packaging\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/46/19/c5ab91b1b05cfe63cccd5cfc971db9214c6dd6ced54e33c30d5af1d2bc43/packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->transformers) (2.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Collecting click\n",
      "  Downloading http://mirrors.tencentyun.com/pypi/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 2.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/envs/tensorflow_py3/lib/python3.6/site-packages (from packaging->transformers) (2.4.6)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893259 sha256=2da07ef9dd576c170f5ca18d0e8e3aa3ea965268a01923ac092c76152d2f4b04\n",
      "  Stored in directory: /home/tione/.cache/pip/wheels/84/1c/87/2507bc9fb552580e00a36152bae7ffb98e9280248c698b0959\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, regex, filelock, click, sacremoses, dataclasses, sentencepiece, packaging, transformers\n",
      "Successfully installed click-7.1.2 dataclasses-0.7 filelock-3.0.12 packaging-20.4 regex-2020.6.8 sacremoses-0.0.43 sentencepiece-0.1.92 tokenizers-0.7.0 transformers-2.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-88f4be6da940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load dataset, tokenizer, model from pretrained model/vocabulary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_datasets'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import *\n",
    "\n",
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')\n",
    "\n",
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)\n",
    "\n",
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)\n",
    "\n",
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)\n",
    "\n",
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')\n",
    "\n",
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
