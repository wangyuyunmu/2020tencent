{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.datasets import imdb\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "\n",
    "\n",
    "class Self_Attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(Self_Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # 为该层创建一个可训练的权重\n",
    "        #inputs.shape = (batch_size, time_steps, seq_len)\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=(3,input_shape[2], self.output_dim),\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True)\n",
    "\n",
    "        super(Self_Attention, self).build(input_shape)  # 一定要在最后调用它\n",
    "\n",
    "    def call(self, x):\n",
    "        WQ = K.dot(x, self.kernel[0])\n",
    "        WK = K.dot(x, self.kernel[1])\n",
    "        WV = K.dot(x, self.kernel[2])\n",
    "\n",
    "        print(\"WQ.shape\",WQ.shape)\n",
    "\n",
    "        print(\"K.permute_dimensions(WK, [0, 2, 1]).shape\",K.permute_dimensions(WK, [0, 2, 1]).shape)\n",
    "\n",
    "\n",
    "        QK = K.batch_dot(WQ,K.permute_dimensions(WK, [0, 2, 1]))\n",
    "\n",
    "        QK = QK / (self.output_dim**0.5)\n",
    "\n",
    "        QK = K.softmax(QK)\n",
    "\n",
    "        print(\"QK.shape\",QK.shape)\n",
    "\n",
    "        V = K.batch_dot(QK,WV)\n",
    "\n",
    "        return V\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "\n",
    "        return (input_shape[0],input_shape[1],self.output_dim)\n",
    "\n",
    "max_features = 20000\n",
    "\n",
    "\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "#标签转换为独热码\n",
    "y_train, y_test = pd.get_dummies(y_train),pd.get_dummies(y_test)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "\n",
    "#%%数据归一化处理\n",
    "\n",
    "maxlen = 64\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n",
    "#%%\n",
    "\n",
    "batch_size = 32\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.layers import *\n",
    "\n",
    "\n",
    "\n",
    "S_inputs = Input(shape=(64,), dtype='int32')\n",
    "\n",
    "embeddings = Embedding(max_features, 128)(S_inputs)\n",
    "\n",
    "\n",
    "O_seq = Self_Attention(128)(embeddings)\n",
    "\n",
    "\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "\n",
    "outputs = Dense(2, activation='softmax')(O_seq)\n",
    "\n",
    "\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "# try using different optimizers and different optimizer configs\n",
    "opt = Adam(lr=0.0002,decay=0.00001)\n",
    "loss = 'categorical_crossentropy'\n",
    "model.compile(loss=loss,\n",
    "\n",
    "             optimizer=opt,\n",
    "\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "#%%\n",
    "print('Train...')\n",
    "\n",
    "h = model.fit(x_train, y_train,\n",
    "\n",
    "         batch_size=batch_size,\n",
    "\n",
    "         epochs=5,\n",
    "\n",
    "         validation_data=(x_test, y_test))\n",
    "\n",
    "plt.plot(h.history[\"loss\"],label=\"train_loss\")\n",
    "plt.plot(h.history[\"val_loss\"],label=\"val_loss\")\n",
    "plt.plot(h.history[\"acc\"],label=\"train_acc\")\n",
    "plt.plot(h.history[\"val_acc\"],label=\"val_acc\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#model.save(\"imdb.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py3",
   "language": "python",
   "name": "conda_tensorflow_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
